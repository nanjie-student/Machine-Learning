{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pandas2.jpeg\" width=\"400\" height=\"400\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures in pandas\n",
    " \n",
    "There are three main data structures in pandas:\n",
    "\n",
    "* Series\n",
    "* DataFrame\n",
    "* Panel\n",
    " \n",
    "The DataFrame represents the entire spreadsheet, whereas the Series is a single column of the DataFrame. \n",
    "\n",
    "A Pandas DataFrame can also be thought of as a dictionary or collection of Series objects.\n",
    "\n",
    "The data sets are first read into these dataframes and then various operations (e.g. group by, aggregation etc.) can be applied very easily to its columns. Row label indexes and column labels can be specified along with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T20:06:01.801343Z",
     "start_time": "2020-10-14T20:06:00.430258Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set some pandas options fro controlling output\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas DataFrame Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame\n",
    "A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns.\n",
    "\n",
    "##### Features of DataFrame\n",
    "- Potentially columns are of different types\n",
    "- Size – Mutable\n",
    "- Labeled axes (rows and columns)\n",
    "\n",
    "\n",
    "pandas.DataFrame(data, index, dtype, copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame\n",
    "A pandas DataFrame can be created using various inputs like −\n",
    "\n",
    "- Lists\n",
    "- dict\n",
    "- Series\n",
    "- Numpy ndarrays\n",
    "- Another DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Empty dataFrame\n",
    "df = pd.DataFrame()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a single list \n",
    "data = [1,2,3,4,5]\n",
    "df = pd.DataFrame(data)\n",
    "print (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists\n",
    "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
    "df = pd.DataFrame(data,columns=['Name','Age'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data,columns=['Name','Age'],dtype=float)\n",
    "print(df)\n",
    "\n",
    "# NOTE: Observe, the dtype parameter changes the type of Age column to floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from Dict \n",
    "\n",
    "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# NOTE: Observe the values 0,1,2,3. They are the default index assigned \n",
    "#to each using the function range(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create an indexed DataFrame \n",
    "\n",
    "data = {'Name':['Harry', 'John', 'Steve', 'Ron'],'Age':[38,34,39,42]}\n",
    "df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "print(df.index)\n",
    "\n",
    "# NOTE: Observe, the index parameter assigns an index to each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from List of Dicts\n",
    "data = [{'x': 1, 'y': 2},{'x': 5, 'y': 10, 'z': 20}]\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# NOTE: Observe, NaN (Not a Number) is appended in missing areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with row indices and column indices\n",
    "data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}]\n",
    "\n",
    "#With two column indices, values same as dictionary keys\n",
    "df1 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The order is not guaranteed because Python dictionaries are not ordered. \n",
    "If we want an #ordered dictionary, we need to use the *OrderedDict* from \n",
    "the **collections** module\n",
    "'''\n",
    "from collections import OrderedDict\n",
    "\n",
    "ord_dict = OrderedDict([('Name', ['Roseline', 'william']),('Occupation', ['Chemist', 'data Analyst']), ('Age', [37, 47])])\n",
    "ord_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ord_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_df = pd.DataFrame(ord_dict)\n",
    "ord_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame with row index label\n",
    "dict1 = {'Occupation': ['Chemist', 'data Analyst'], 'Age': [37, 47], 'Edu': ['PhD', 'BE']}\n",
    "dict1_df = pd.DataFrame(dict1, index = ['Robin', 'Wiiliam'], columns = ['Occupation', 'Age', 'Edu'])\n",
    "dict1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Some attributes of Series\n",
    "# index, values\n",
    "print(dict1_df.index)\n",
    "print(dict1_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series methods is keys which is an alias for the index attributes\n",
    "print(dict1_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is shape of this DataFrame\n",
    "df1.shape   # it has two rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the index of the DataFrame\n",
    "df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with two series objects and a dictionary\n",
    "s1 = pd.Series(np.arange(1,6,1))\n",
    "s2 = pd.Series(np.arange(6,11,1))\n",
    "df = pd.DataFrame({'a':s1, 'b':s2})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame performs automatic alignment of the data for each Series\n",
    "s3 = pd.Series(np.arange(12,14), index = [2,3])\n",
    "df = pd.DataFrame({'a':s1, 'b':s2, 'c':s3})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selcting columns by column names\n",
    "df[['a', 'b']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just one column\n",
    "df[['a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its is DataFrame, not a series\n",
    "type(df[['a']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with integers as the column names\n",
    "df1 = df.copy()\n",
    "df1.columns = [0,1,2]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes access of the column by name\n",
    "df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of the column with the value of a\n",
    "loc = df.columns.get_loc('a')\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Addition\n",
    "d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n",
    "      'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "# Adding a new column to an existing DataFrame object with column label by passing new series\n",
    "\n",
    "data = [10,20,30]\n",
    "print (\"Adding a new column by passing as Series:\")\n",
    "df['three']=pd.Series(data,index=['a','b','c'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Deletion\n",
    "d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), \n",
    "     'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']), \n",
    "     'three' : pd.Series([10,20,30], index=['a','b','c'])}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "print('Original data Frame')\n",
    "print('\\n')\n",
    "print(df)\n",
    "# using del function\n",
    "print (\"Deleting the first column using DEL function:\")\n",
    "del df['one']\n",
    "print (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing and Selecting data\n",
    "\n",
    "The Python and NumPy indexing operators \"[ ]\" and attribute operator \".\" provide quick and easy access to Pandas data structures across a wide range of use cases. However, since the type of the data to be accessed isn’t known in advance, directly using standard operators has some optimization limits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Indexing |   Description   |\n",
    "|:---|:---|\n",
    "|   .loc() | Label Based    |\n",
    "|   .iloc() | Integer Based    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .loc()\n",
    "Pandas provide various methods to have purely label based indexing. When slicing, the start bound is also included. Integers are valid labels, but they refer to the label and not the position.\n",
    "\n",
    ".loc() has multiple access methods like −\n",
    "\n",
    "A single scalar label\n",
    "A list of labels\n",
    "A slice object\n",
    "A Boolean array\n",
    "loc takes two single/list/range operator separated by ','. The first one indicates the row and the second one indicates columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T23:12:41.914179Z",
     "start_time": "2020-10-18T23:12:41.853654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given DataFrame\n",
      "          A         B         C         D\n",
      "a  0.128600 -0.297208  0.318032 -0.642987\n",
      "b -0.323376 -1.216343  0.761163 -2.121079\n",
      "c -0.067477 -0.454824  0.594597  2.461072\n",
      "d -0.366417 -0.832765  1.353210  0.423727\n",
      "e -0.946492  0.527469 -0.678439 -1.517093\n",
      "f  1.465094 -0.950003 -0.726483  0.949261\n",
      "g -2.166426 -0.340402 -0.441210  1.863917\n",
      "h -0.027525 -0.034871  0.729898 -0.635639\n",
      "\n",
      "\n",
      "select all rows for a specific column, e.g. column A\n",
      "a    0.128600\n",
      "b   -0.323376\n",
      "c   -0.067477\n",
      "d   -0.366417\n",
      "e   -0.946492\n",
      "f    1.465094\n",
      "g   -2.166426\n",
      "h   -0.027525\n",
      "Name: A, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(8, 4),\n",
    "index = ['a','b','c','d','e','f','g','h'], columns = ['A', 'B', 'C', 'D'])\n",
    "print('Given DataFrame')\n",
    "print(df)\n",
    "print('\\n')\n",
    "print('select all rows for a specific column, e.g. column A')\n",
    "print(df.loc[:,'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Select all rows for multiple columns, say list')\n",
    "print(df.loc[:,['A','C']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Select few rows for multiple columns, say list[]')\n",
    "print(df.loc[['a','b','f','h'],['A','C']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select range of rows for all columns\n",
    "print(df.loc['a':'h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting values with a boolean array\n",
    "print(df.loc['a']>0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .iloc()\n",
    "Pandas provide various methods in order to get purely integer based indexing. Like python and numpy, these are 0-based indexing.\n",
    "\n",
    "The various access methods are as follows −\n",
    "\n",
    "- An Integer\n",
    "- A list of integers\n",
    "- A range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns for a specific rows\n",
    "print(df.iloc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T23:19:35.843527Z",
     "start_time": "2020-10-18T23:19:35.834871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          C         D\n",
      "b  0.761163 -2.121079\n",
      "c  0.594597  2.461072\n",
      "d  1.353210  0.423727\n",
      "e -0.678439 -1.517093\n"
     ]
    }
   ],
   "source": [
    "# Integer slicing\n",
    "print(df.iloc[1:5, 2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing through list of values\n",
    "print(df.iloc[[1, 3, 5], [1, 3]])\n",
    "print('\\n')\n",
    "print(df.iloc[1:3, :])\n",
    "print('\\n')\n",
    "print(df.iloc[:,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row Selection, Addition, and Deletion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice rows\n",
    "\n",
    "print(df[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of rows\n",
    "df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "df2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])\n",
    "print(df2)\n",
    "print('\\n')\n",
    "df = df.append(df2)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows\n",
    "#Use index label to delete or drop rows from a DataFrame. \n",
    "#If label is duplicated, \n",
    "#then multiple rows will be dropped.\n",
    "\n",
    "#If you observe, in the above example, \n",
    "#the labels are duplicate. Let us drop a label and \n",
    "#will see how many rows will get dropped.\n",
    "\n",
    "\n",
    "df = df.drop(0)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series with 10 random numbers\n",
    "s = pd.Series(np.random.randn(10))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axes: Returns the list of the labels of the series.\n",
    "print (\"The axes are:\")\n",
    "print(s.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty: Returns the Boolean value saying whether the Object is empty or not. \n",
    "# True indicates that the object is empty.\n",
    "print (\"Is the Object empty?\")\n",
    "print(s.empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values: Returns the actul data in the series as an array\n",
    "\n",
    "print (\"The actual data series is:\")\n",
    "print(s.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head and Tail \n",
    "To view a small sample of a Series or the DataFrame object, use the head() and the tail() methods.\n",
    "\n",
    "head() returns the first n rows(observe the index values). \n",
    "The default number of elements to display is **five**, \n",
    "but you may pass a custom number.\n",
    "\n",
    "tail() returns the last n rows(observe the index values). \n",
    "The default number of elements to display is **five**, but you may pass a custom number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "\n",
    "print(s.head())\n",
    "print('\\n')\n",
    "print (\"The first four rows of the data series:\")\n",
    "print(s.head(4))\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail\n",
    "\n",
    "print(s.tail())\n",
    "print('\\n')\n",
    "print (\"The last four rows of the data series:\")\n",
    "print(s.tail(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame \n",
    "d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Smith','Jack']),\n",
    "   'Age':pd.Series([25,26,25,23,30,29,23]),\n",
    "   'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8])}\n",
    "\n",
    "#Create a DataFrame\n",
    "df = pd.DataFrame(d)\n",
    "print (\"Our data series is:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T (Transpose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The transpose of the data series is:\")\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes\n",
    "#Returns the data type of each column.\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: Returns a tuple representing the dimensionality of the \n",
    "#DataFrame. Tuple (a,b), where a represents the number of rows and b represents the number of \n",
    "#columns.\n",
    "\n",
    "print (\"The shape of the object is:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size: Number of elements in the DataFrame\n",
    "print (\"The total number of elements in the object is:\")\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values: Returns the actual data in the DataFrame\n",
    "print (\"The actual data in our data frame is:\")\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head and tail\n",
    "print (\"The first four rows of the data frame is:\")\n",
    "print (df.head(4))\n",
    "print('\\n')\n",
    "\n",
    "print (\"The last four rows of the data frame is:\")\n",
    "print (df.tail(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() Returns the sum of the values for the requested axis. By default, axis is index (axis=0).\n",
    "\n",
    "print(df.sum())\n",
    "\n",
    "# Each individual column is added individually \n",
    "# (Strings are appended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis = 1\n",
    "print(df.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std and variance\n",
    "\n",
    "print(\"mean:\")\n",
    "print(df.mean())\n",
    "print('\\n')\n",
    "print(\"standard deviation:\")\n",
    "print(df.std())\n",
    "print('\\n')\n",
    "print(\"variance:\")\n",
    "print(df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Data\n",
    "\n",
    "**describe:** The describe() function computes a summary of \n",
    "statistics pertaining to the DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row and Column wise function \n",
    "\n",
    "Arbitrary functions can be applied along the axes of a DataFrame or Panel using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument. By default, the operation performs column wise, taking each column as an array-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(5,3),columns=['x1','x2','x3'])\n",
    "df.apply(np.mean)\n",
    "print (df.apply(np.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By passing axis paramter, operations can be performed row wise\n",
    "print(df.apply(np.mean,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.apply(lambda x: x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration\n",
    "\n",
    "The behavior of basic iteration over Pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. Other data structures, like DataFrame and Panel, follow the dict-like convention of iterating over the keys of the objects.\n",
    "\n",
    "In short, basic iteration (for i in object) produces −\n",
    "\n",
    "Series − values\n",
    "\n",
    "DataFrame − column labels\n",
    "\n",
    "Panel − item labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating a DataFrame\n",
    "# Iterating a DataFrame gives column names.\n",
    "\n",
    "for col in df:\n",
    "    print(col)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over the rows of the DataFrame, we can use the following functions −\n",
    "for key,value in df.iteritems():\n",
    "   print key,value\n",
    "iteritems() − to iterate over the (key,value) pairs\n",
    "\n",
    "iterrows() − iterate over the rows as (index,series) pairs\n",
    "\n",
    "itertuples() − iterate over the rows as namedtuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in df.iteritems():\n",
    "   print (key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterrows()\n",
    "#iterrows() returns the iterator yielding each index value \n",
    "#along with a series containing the data in each row.\n",
    "\n",
    "for row_index,row in df.iterrows():\n",
    "   print(row_index,row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### itertuples()\n",
    "**itertuples()** method will return an iterator yielding a \n",
    "named tuple for each row in the DataFrame. \n",
    "The first element of the tuple will be the row’s corresponding \n",
    "index value, while the remaining values are the row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting\n",
    "There are two kinds of sorting available in Pandas. They are −\n",
    "\n",
    "- By label\n",
    "- By Actual Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.random.randn(10,2),index=[1,4,6,2,3,5,9,8,0,7],columns=['x1','x2'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "# NOTE: In unsorted_df, the labels and the values are unsorted. \n",
    "# Let us see how these can be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Label\n",
    "Using the sort_index() method, by passing the axis arguments \n",
    "and the order of sorting, DataFrame can be sorted. By default, \n",
    "sorting is done on row labels in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_index()\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order of sorting\n",
    "By passing the Boolean value to ascending parameter, the order of the sorting can be controlled. Let us consider the following example to understand the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_index(ascending = False)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the Columns\n",
    "By passing the axis argument with a value 0 or 1, the sorting can be done on the column labels. By default, axis=0, sort by row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df=df.sort_index(axis=1)\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Value\n",
    "Like index sorting, sort_values() is the method for sorting by values. It accepts a 'by' argument which will use the column name of the DataFrame with which the values are to be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='x1')\n",
    "print(sorted_df)\n",
    "\n",
    "# NOTE: Observe, col1 values are sorted and the respective \n",
    "#x2 value and row index \n",
    "# will alter along with x1. Thus, they look unsorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Access\n",
    "Columns can be selected using the attribute operator '.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(df.x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and Preparation\n",
    "Below are the steps involved to understand, clean and prepare the data for data analysis or building the predictive model:\n",
    "\n",
    "1. Variable Identification\n",
    "2. Univariate Analysis\n",
    "3. Bi-variate Analysis\n",
    "4. Missing values treatment\n",
    "5. Outlier treatment\n",
    "6. Variable transformation\n",
    "7. Variable creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Functions\n",
    "Statistical methods help in the understanding and analyzing the behavior of data. we will learn few of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent_change\n",
    "Series, DatFrames and Panel, all have the function pct_change(). This function compares every element with its prior element and computes the change percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([1,2,3,4,5,4])\n",
    "\n",
    "print('Given Series:\\n', x)\n",
    "print ('percentage change:\\n', x.pct_change())\n",
    "print('\\n')\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(5, 2))\n",
    "print('data:\\n ',df)\n",
    "print('\\n')\n",
    "print(df.pct_change())\n",
    "\n",
    "# NOTE: By default, the pct_change() operates on columns; if \n",
    "# you want to apply the same row wise, then use axis=1() argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance\n",
    "s1 = pd.Series(np.random.randn(10))\n",
    "s2 = pd.Series(np.random.randn(10))\n",
    "print (s1.cov(s2))\n",
    "print('\\n')\n",
    "#Covariance method when applied on a DataFrame, \n",
    "#computes cov between all the columns.\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'])\n",
    "print (df['a'].cov(df['b']))\n",
    "print('\\n')\n",
    "print(df.cov())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "Correlation shows the linear relationship between any two array of values (series). There are multiple methods to compute the correlation like pearson(default), spearman and kendall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "print(df['a'].corr(df['b']))\n",
    "\n",
    "print (df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data\n",
    "Missing data is a problem in real life scenarios. Areas like machine learning and data mining face severe issues in the accuracy of their model predictions because of poor quality of data caused by missing values. In these areas, missing value treatment is a major point of focus to make their models more accurate and valid.\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "- Ignore the Missing Value During Analysis:  This is usually done when class label is missing ( assuming the mining task involves classification). This method is not very effective, unless the record contains several attributes with missing values. It is especially poor when the percentage of missing values per attribute varies considerably.\n",
    "\n",
    "\n",
    "- Fill in the missing value manually: In general, this approach is time consuming and may not be feasible given a large data set with many missing values\n",
    "\n",
    "\n",
    "- Use a global constant/mean or median to fill in the missing value: Replace all missing feature values by the same constant/mean or median of the attribute\n",
    "\n",
    "\n",
    "- Use the most probable value to fill in the missing value: This may be determined with regression, inference-based tools using a Bayesian formalism, or decision tree induction. \n",
    "\n",
    "\n",
    "**Note** it is as important to avoid adding bias and distortion\n",
    "to the data as it is to make the information available.\n",
    "\t\n",
    "\t• bias is added when a wrong value is filled-in\n",
    "\n",
    "No matter what techniques you use to conquer the problem, it\n",
    "comes at a price. The more guessing you have to do, the further\n",
    "away from the real data the database becomes. Thus, in turn, it\n",
    "can affect the accuracy and validation of the mining results. \n",
    "\n",
    "\n",
    "Here we will see how we can handle missing values using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f',\n",
    "'h'],columns=['one', 'two', 'three'])\n",
    "\n",
    "df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values\n",
    "\n",
    "Pandas provides the **isnull** and **notnull()** functions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull())  # notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do one column also\n",
    "\n",
    "print(df['one'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of missing values in each column\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print('\\n')\n",
    "\n",
    "# Calculate the total number of missing values in each row\n",
    "print(df.isnull().sum(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations with Missing Data\n",
    "\n",
    "- When summing data, NA will be treated as Zero\n",
    "- If the data are all NA, then the result will be NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one column\n",
    "print(df['one'].sum())\n",
    "print('\\n')\n",
    "\n",
    "# DataFrame\n",
    "print(df.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Missing Values\n",
    "If you want to simply exclude the missing values, then use the dropna function along with the axis argument. By default, axis=0, i.e., along row, which means that if any value within a row is NA then the whole row is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dropna(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
    "                     [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data.dropna()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4] = np.nan\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing how='all' will only drop rows that are all NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=1, how='all')  # axis = 1 drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to keep only rows containing a certain number of observations. You can indicate this with the thresh argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Missing data\n",
    "Rather than filtering out missing data (and potentially discarding other data along with it), you may want to fill in the “holes” in any number of ways. \n",
    "\n",
    "Pandas provides **fillna** function to fill in NA values with non-null data in various ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with a scalar Value\n",
    "\n",
    "print (\"NaN replaced with '0':\")\n",
    "print(data.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA Foward and Backward\n",
    "\n",
    "| Method |   ACtion   |\n",
    "|:---|:---|\n",
    "|   pad/fill | Fill methods forward   |\n",
    "|   bfill/backfill | Fill methods Backward    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.fillna(method='pad')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.fillna(method='backfill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(7, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:4, 1] = np.nan\n",
    "df.iloc[:2, 2] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling *fillna* with a dict, you can use a different fill value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.fillna({1: 0.5, 2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With *fillna* you can do lots of other things with a little creativity. For example, you might pass the mean or median value of a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T20:13:35.436089Z",
     "start_time": "2020-10-14T20:13:35.413403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  2\n",
       "2  3\n",
       "3  4\n",
       "5  5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= pd.DataFrame([1,2,3,4,1,5,1,3])\n",
    "x.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T20:19:25.133031Z",
     "start_time": "2020-10-14T20:19:25.125964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T20:21:26.214689Z",
     "start_time": "2020-10-14T20:21:26.208000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],'k2': [1, 1, 2, 3, 3, 4, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method *duplicated* returns a boolean Series indicating whether each row is a duplicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, *drop_duplicates* returns a DataFrame where the duplicated array is False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these methods by default consider all of the columns; alternatively, you can specify any subset of them to detect duplicates. Suppose we had an additional column of values and wanted to filter duplicates only based on the 'k1' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'] = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing Values\n",
    "\n",
    "*replace* provides a simpler and more flexible way to do replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -999 values might be sentinel values for missing data. To replace these with NA values that pandas understands, we can use replace, producing a new Series (unless you pass inplace=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to replace multiple values at once, you instead pass a list and then the substitute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([-999, -1000], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a different replacement for each value, \n",
    "# pass a list of substitutes:\n",
    "\n",
    "data.replace([-999, -1000], [np.nan, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument passed can also be a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({-999: np.nan, -1000: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization and Binning\n",
    "Continuous data is often discretized or otherwise separated into “bins” for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:53:37.204306Z",
     "start_time": "2020-10-15T19:53:37.199552Z"
    }
   },
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To do so, you have to use cut, a function in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:53:39.731865Z",
     "start_time": "2020-10-15T19:53:39.727702Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = [18, 25, 35, 60, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:53:41.044300Z",
     "start_time": "2020-10-15T19:53:40.911766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]\n",
       "Length: 12\n",
       "Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = pd.cut(ages,bins)\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object pandas returns is a special Categorical object. The output you see describes the bins computed by pandas.cut. You can treat it like an array of strings indicating the bin name; internally it contains a categories array specifying the dis‐ tinct category names along with a labeling for the ages data in the codes attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:54:29.905980Z",
     "start_time": "2020-10-15T19:54:29.898628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:54:34.753018Z",
     "start_time": "2020-10-15T19:54:34.745848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]],\n",
       "              closed='right',\n",
       "              dtype='interval[int64]')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with mathematical notation for intervals, a parenthesis means that the side is open, while the square bracket means it is closed (inclusive). You can change which side is closed by passing right=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:54:40.067708Z",
     "start_time": "2020-10-15T19:54:40.050796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)]\n",
       "Length: 12\n",
       "Categories (4, interval[int64]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(ages, [18, 26, 36, 61, 100], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:54:44.381298Z",
     "start_time": "2020-10-15T19:54:44.356319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 25]     5\n",
       "(35, 60]     3\n",
       "(25, 35]     3\n",
       "(60, 100]    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass your own bin names by passing a list or array to the labels option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(ages, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass an integer number of bins to cut instead of explicit bin edges, it will com‐ pute equal-length bins based on the minimum and maximum values in the data. Consider the case of some uniformly distributed data chopped into fourths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(data, 4, precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting and Filtering Outliers\n",
    "- Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set\n",
    "\n",
    "\n",
    "- Data points inconsistent with the majority of data\n",
    "\n",
    "\n",
    "- Outlier detection can be used for fraud detection or data cleaning\n",
    "\n",
    "\n",
    "Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n",
    "\n",
    "\n",
    "- It increases the error variance and reduces the power of statistical tests\n",
    "\n",
    "\n",
    "- If the outliers are non-randomly distributed, they can decrease normality\n",
    "\n",
    "\n",
    "- They can bias or influence estimates that may be of substantive interest\n",
    "\n",
    "\n",
    "- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to detect outliers\n",
    "\n",
    "Most commonly used method to detect outliers is visualization. \n",
    "\n",
    "We use various visualization methods, like Box-plot, Histogram, Scatter plot, clustering, curve fitting. \n",
    "\n",
    "\n",
    "Some analysts also various thumb rules to detect outliers. Some of them are:\n",
    "\n",
    "- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    "\n",
    "\n",
    "- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    "\n",
    "- Data points, three or more standard deviation away from mean are considered outlier\n",
    "\n",
    "\n",
    "- Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n",
    "\n",
    "\n",
    "\n",
    "#### How to remove them \n",
    "Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n",
    "\n",
    "Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.random.randn(1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to find values in one of the columns exceeding 3 in absolute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = data[2]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col[np.abs(col)>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select all rows having a value exceeding 3 or –3, you can use the any method on a boolean DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(np.abs(data) > 3).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be set based on these criteria. Here is code to cap values outside the inter‐ val –3 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[np.abs(data) > 3] = np.sign(data) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Indicator/Dummy Variables\n",
    "Another type of transformation for statistical modeling or machine learning applica‐ tions is converting a categorical variable into a “dummy” or “indicator” matrix. If a column in a DataFrame has k distinct values, you would derive a matrix or Data‐ Frame with k columns containing all 1s and 0s. pandas has a get_dummies function for doing this, though devising one yourself is not difficult. \n",
    "\n",
    "When you’re using statistics or machine learning tools, you’ll often transform catego‐ rical data into dummy variables, also known as one-hot encoding. This involves creat‐ ing a DataFrame with a column for each distinct category; these columns contain 1s for occurrences of a given category and 0 otherwise.\n",
    "\n",
    "Let’s return to an earlier example DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:49:59.521428Z",
     "start_time": "2020-10-15T19:49:59.483829Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],'data1': range(6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:50:01.612043Z",
     "start_time": "2020-10-15T19:50:01.598126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key      object\n",
       "data1     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:50:03.366651Z",
     "start_time": "2020-10-15T19:50:03.346325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key  data1\n",
       "0   b      0\n",
       "1   b      1\n",
       "2   a      2\n",
       "3   c      3\n",
       "4   a      4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:50:04.978947Z",
     "start_time": "2020-10-15T19:50:04.937309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1  key_a  key_b  key_c\n",
       "0      0      0      1      0\n",
       "1      1      0      1      0\n",
       "2      2      1      0      0\n",
       "3      3      0      0      1\n",
       "4      4      1      0      0\n",
       "5      5      0      1      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:51:19.349793Z",
     "start_time": "2020-10-15T19:51:19.337054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  0\n",
       "1  0  1  0\n",
       "2  1  0  0\n",
       "3  0  0  1\n",
       "4  1  0  0\n",
       "5  0  1  0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:51:39.680636Z",
     "start_time": "2020-10-15T19:51:39.672406Z"
    }
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['key'], prefix='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:51:40.617117Z",
     "start_time": "2020-10-15T19:51:40.606183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   key_a  key_b  key_c\n",
       "0      0      1      0\n",
       "1      0      1      0\n",
       "2      1      0      0\n",
       "3      0      0      1\n",
       "4      1      0      0\n",
       "5      0      1      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummy = df[['data1']].join(dummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummy1 = df.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful recipe for statistical applications is to combine get_dummies with a discreti‐ zation function like cut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.random.rand(10)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(pd.cut(values, bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Manipulation\n",
    "Python has long been a popular raw data manipulation language in part due to its ease of use for string and text processing. Most text operations are made simple with the string object’s built-in methods. \n",
    "\n",
    "\n",
    "For more complex pattern matching and text manipulations, regular expressions may be needed. pandas adds to the mix by enabling you to apply string and regular expressions concisely on whole arrays of data, additionally handling the annoyance of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'a,b,  guido'\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = val.split(',')\n",
    "val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split is often combined with strip to trim whitespace (including line breaks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pieces = [x.strip() for x in val.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These substrings could be concatenated together with a two-colon delimiter using addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second, third = str_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first + '::' + second + '::' + third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this isn’t a practical generic method. A faster and more Pythonic way is to pass a list or tuple to the join method on the string '::':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods are concerned with locating substrings. Using Python’s in keyword is the best way to detect a substring, though index and find can also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'::'.join(str_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'guido' in val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.index(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace will substitute occurrences of one pattern for another. It is commonly used to delete patterns, too, by passing an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.replace(',', '::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument |   Description|\n",
    "|:---|:---|\n",
    "|   count | Return the number of non-overlapping occurrences of substring in the string. |\n",
    "|   endswith |  ReturnsTrueif string ends with suffix. |\n",
    "|   startswith |  ReturnsTrueif string starts with prefix. |\n",
    "|   join |  Use string as delimiter for concatenating a sequence of other strings. |\n",
    "|   index | Return position of first character in substring if found in the string; raises ValueError if not found. |\n",
    "|   find |  Return position of first character of first occurrence of substring in the string; like index, but returns –1\n",
    "if not found. |\n",
    "|   replace|  Replace occurrences of string with another string. |\n",
    "|  strip|  Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively) for each element.|\n",
    "|   upper|  Convert alphabet characters to uppercase.|\n",
    "|  lower|  Convert alphabet characters to lowercase.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and Reshaping\n",
    "\n",
    "Once the data is tidied up,it will likely that we will then need to use this data either to combine multiple sets of data, or to reorganize the data. \n",
    "\n",
    "Here we will discuss combination and reshaping of data. Combination of data in pandas is performed by concatenating two sets of data, where data is combined simply along either axes but without regard to the relationships in the data. Or data can be combined using relationships in the data by using a pandas capability referred to as merging, which provides join operations that are similar to those in may relational databases. \n",
    "\n",
    "We will discuss various reshaping techniques like pivoting, stacking, and unstacking, and melting of data. \n",
    "\n",
    "Pivoting allows to restructure data similarly to how spreadsheets pivot data by creating new index levels and moving data into columns based upon values (or vice-versa). \n",
    "\n",
    "Stacking and unstacking are similar to pivoting, but allow us to pivot data organized with multiple levels of indexes.\n",
    "\n",
    "Finally, melting allows to restructure data into unique ID-variable-measurement combinations that are or required for many statistical analyses. Following concepts of combining and reshaping are available and will be discussed.\n",
    "\n",
    "- Concatenation\n",
    "- Merging and joining\n",
    "- Pivots\n",
    "- Stacking/unstacking\n",
    "- Melting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating\n",
    "\n",
    "Concatenating is the process of either adding rows to the end of an Series or DataFrame. The operation is performed via the function *concat*. Again the function will perform the operation on a specific axis. The general syntax is to pass a list of objects to *concat()* function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.arange(1,5))\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(np.arange(5,9))\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1,s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two DataFrame objects can also be cocatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(9).reshape(3,3), columns = ['x','y','z'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(9,18).reshape(3,3), columns = ['x','y','z'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of concatenating the two Dataframe objects will first identify the set of columns formed by aligning the labels in the columns, effectively determining the union of the column names. The resulting DataFrame object will then consists of columns, and columns with identical names will not be duplicated \n",
    "\n",
    "Rows will be then be added to the result, in the order of the each of the objects passed to function *concat()*. If a column in the result does not exist in the object being copied, NaN values will be filled in those locations. Duplicate row index labels can occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(9).reshape(3,3), columns = ['a','b','c'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(9,18).reshape(3,3), columns = ['a','c','d'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two objects, but create an index using the given keys\n",
    "\n",
    "df3 = pd.concat([df1,df2])\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*concat()* function allows to specify the axis on which to apply the concatenation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2], axis =1)  # column wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the resultant dataframe contains duplicate columns. The concatenation first aligns by the row index labels of each DataFrame and then fills in the columns from the first DataFrame and then then the second. The columns are not aligned and result in duplicate values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same rules of alignment and filling on NaN values apply in thsi case except that they are applied to the rows index labels. The following example demonstrates a concatenation along the columns axis with two DataFrames that have row index labebsl in common (2 and 3) along with disjoint rows(0 in df1 and 4 in df3). Additionally, some of the columns in df3 overlap with df1(a) as well as being disjoint(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(np.arange(20,26).reshape(3,2), columns = ['a','d'], index = [2,3,4])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df3], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concatenation of two or more DataFrame actually performs an outer join along the index labels on the axis opposite to the one specified. This makes the result of the concatenation similar to having performed a union of those index labels, and then data is filled based on the alignment of those labels to the source objects. \n",
    "\n",
    "\n",
    "The type of join can be changed to an inner join and can be performed by specifying *join= 'inner'* as the parameter. The inner join then logically performs the interaction instead of a union. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df3], axis = 1, join = 'inner') # only 2 is the common row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use label groups of data along the columns using the *keys* parameter when applying the concatenation along axis = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2], axis = 1, keys = ['df1', 'df2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different groups can be accessed using the *.ix* process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ix[:,'df2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*append()* method concatenate the two specified DataFrame long the row index labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a concatenation on axis =1, the index labels in the rows are copied without consideration of the creation of duplicates, and the column labels are joined in a manner which ensures no duplicate column name in included in the result. \n",
    "\n",
    "If you would like to ensure that the resulting index does not have duplicates but preserves all of the rows, you can use the *ignore_index = True* parameter. This returns the same result except with new *Int64Index* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(df2,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging and joining data\n",
    "\n",
    "pandas provides functionality of merging the panadas objects with databas like join operations using the *merge()* function and *merge* metod of a Dataframe objects.\n",
    "\n",
    "A merge combines the data of two pandas objects by finding matching values in one or more columns or row indexes. It then returns a new object that represents a combination of the data from both based on relational-databases-like join applied to those values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging/Joining\n",
    "Pandas has full-featured, high performance joining operations similar to SQL.\n",
    "\n",
    "Pandas provides a single function, *merge*, as the entry point for all standard database join operations between DataFrame objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'id':[1,2,3,4,5],\n",
    "         'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'],\n",
    "         'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n",
    "df2 = pd.DataFrame(\n",
    "         {'id':[1,2,3,4,5],\n",
    "         'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'],\n",
    "         'subject_id':['sub2','sub4','sub3','sub6','sub5']})\n",
    "print(df1)\n",
    "print('\\n')\n",
    "\n",
    "print(df2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Two DataFrames on a key\n",
    "print(pd.merge(df1,df2,on = 'id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Two DataFrames on Multiple Keys\n",
    "print(pd.merge(df1,df2,on=['id','subject_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.DataFrame({'CustomerID': [10,11], 'Name':['John', 'Jenny'],'Address':[\"address of john\", 'address of jenny']})\n",
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "orders = pd.DataFrame({'CustomerID': [10,11,10], 'Amount': [1000, 2000,5000]})\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would like to know the amount of the order \n",
    "# each customer spent on each order  \n",
    "\n",
    "customers.merge(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what pandas has done is the following:\n",
    "\n",
    "1. Determines the columns in both customers and orders with common labels. These columns are treated as the keys to perform the join\n",
    "\n",
    "2. It creates a new DataFrame whose columns are the labels from the keys identified from the keys identified in step 1. followed by all of the non-key labels from both objects\n",
    "\n",
    "3. It matches values in the key columns of both DataFrame objects\n",
    "\n",
    "4. It then creates a row in the result for each set of matching labels\n",
    "\n",
    "5. It then copies the data from those matching rows from each source object into that respective row and columns of the result\n",
    "\n",
    "6. it assigns a new Int64Index to the result\n",
    "\n",
    "\n",
    "The join in a merge can use values from multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_data = {'key1':['a','b','c'], 'key2':['x','y','z'], 'lval1':['5','6','7']}\n",
    "left = pd.DataFrame(left_data,index = [0,1,2])\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_data = {'key1':['a','b','c'], 'key2':['x','a','z'], 'lval1':['1','2','3']}\n",
    "\n",
    "right = pd.DataFrame(right_data, index = [1,2,3])\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.merge(right, on = 'key1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.merge(right, on=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns specified with on need to exist in both DataFrames. If you would like to merge based on columns with different names in each object, you can use the left_on and right_on parameters, passing the name or names of columns to each respective parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left,right,left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the join semantics of a merge operation\n",
    "\n",
    "The default type of join performed by the pd.merge is an inner join. The use of another join method, the method of join to be used can be specified using the how parameter of the *merge()* function. The valid options are:\n",
    "\n",
    "- inner: This is the intersection of keys from both DataFrames\n",
    "\n",
    "- outer: This is the union of keys from both DataFrames\n",
    "\n",
    "- left: This only uses keys from the left DataFrame\n",
    "\n",
    "- right: This only uses keys from the right DataFrame\n",
    "\n",
    "#### Merge Using 'how' Argument\n",
    "The **how** argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or the right tables, the values in the joined table will be NA.\n",
    "\n",
    "\n",
    "| Merge Method |   SQL Equivalent   | Description|\n",
    "|:---|:---|:---|\n",
    "|   left | Left Outer Join   | Use keys from left object |\n",
    "|   right | Right Outer Join   | Use keys from right object |\n",
    "|   outer | Full Outer Join   | Use union of keys |\n",
    "|   inner | Inner Join   | Use intersection of keys |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left Join\n",
    "\n",
    "print(pd.merge(df1, df2, on='subject_id', how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Join\n",
    "print(pd.merge(df1, df2, on='subject_id', how='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer Join\n",
    "print(pd.merge(df1, df2, on='subject_id', how='outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "print(pd.merge(df1, df2, on='subject_id', how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.merge(right, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, lsuffix = '_left', rsuffix = '_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy\n",
    "\n",
    "Categorizing a dataset and applying a function to each group, whether an aggregation or transformation, is often a critical component of a data analysis workflow. After loading, merging, and preparing a dataset, you may need to compute group statistics or possibly pivot tables for reporting or visualization purposes. pandas provides a flexible groupby interface, enabling you to slice, dice, and summarize datasets in a natural way.\n",
    "\n",
    "Group operations are described by *split-apply-combine* according to Hadley Wickham, an author of many popular packages for the R programming language\n",
    "\n",
    "Many complex group operations can be performed in pandas. \n",
    "\n",
    "Any **groupby** operation involves one of the following operations on the original object. They are −\n",
    "\n",
    "- **Split** a pandas object into pieces using one or more keys (in the form of functions, arrays, or DataFrame column names)\n",
    "\n",
    "- Calculate group summary statistics, like count, mean, or standard deviation, or a user-defined function\n",
    "\n",
    "- Apply within-group transformations or other manipulations, like normalization, linear regression, rank, or subset selection\n",
    "\n",
    "\n",
    "- Compute pivot tables and cross-tabulations\n",
    "\n",
    "The following diagram demonstrates split-apply-combine process to sum group of numbers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='split-apply-combine.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is similar to the concepts in MapReduce. As in MapReduce job is divided into pieces and distributed to many computers. Each computer then performs analysis on the set of data and calculates a result. The results are then collected and used to make decision.\n",
    "\n",
    "But in pandas, this operation differs in the scope of the data and processing. In pandas, all of the data is in memory of a single system. Because of this, it is limited to that single system's processing capabilities, but tis also makes the data analysis for that scale of data faster and more interactive in nature.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Groups\n",
    "Pandas object can be split into any of their objects. There are multiple ways to split an object like −\n",
    "\n",
    "- obj.groupby('key')\n",
    "- obj.groupby(['key1','key2'])\n",
    "- obj.groupby(key,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "         'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Team')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of calling *groupby()* on a DataFrame is not the actual grouped data, but a DataFrameGroupByobject. The grouping has not actually been performed. This object represents an interim description of the grouping to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of Groups .ngroups\n",
    "grouped.ngroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .groups property will return a Python dictionary whose keys represent the names of each group. The values in the dictionary are an array of the index labels contained within each respective group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the results of grouping\n",
    "#### Iterating Over Groups\n",
    "The GroupBy object supports iteration, generating a sequence of 2-tuples containing the group name along with the chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to print the content of the groups\n",
    "\n",
    "def print_groups (groupobject):\n",
    "    for name, group in groupobject:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multiple keys, the first element in the tuple will be a tuple of key values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1, k2), group in df.groupby(['Team', 'Rank']):\n",
    "    print((k1, k2))\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = dict(list(df.groupby('Team')))\n",
    "pieces['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_groups(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "\n",
    "print (df.groupby(['Team','Year']).groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many items in each group\n",
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of items in each column of each group\n",
    "grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a specific group\n",
    "grouped.get_group('Devils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The head() and tail() methods can be used to return the specified number of items in \n",
    "# each group.\n",
    "\n",
    "grouped.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nth() method will return teh n-th item in each group. \n",
    "# First item\n",
    "grouped.nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second item \n",
    "grouped.nth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.nth(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                   'data1' : np.random.randn(5),\n",
    "                   'data2' : np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to compute the mean of the data1 column using the labels from key1. There are a number of ways to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we had passed multiple arrays as a list, we’d get something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\n",
    "years = np.array([2005, 2005, 2006, 2005, 2006])\n",
    "df['data1'].groupby([states, years]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key1').mean()\n",
    "df.groupby(['key1', 'key2']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the objective in using groupby, a generally useful GroupBy method is size, which returns a Series containing group sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default groupby groups on axis=0, but you can group on any of the other axes. For example, we could group the columns of our example df here by dtype like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped = df.groupby(df.dtypes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, group in grouped:\n",
    "    print(dtype)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Column or Subset of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a GroupBy object created from a DataFrame with a column name or array of column names has the effect of column subsetting for aggregation. This means that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key1')['data1']\n",
    "df.groupby('key1')[['data2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re syntactic sugar for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['data1'].groupby(df['key1'])\n",
    "df[['data2']].groupby(df['key1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially for large datasets, it may be desirable to aggregate only a few columns. For example, in the preceding dataset, to compute means for just the data2 column and get the result as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2'])[['data2']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by this indexing operation is a grouped DataFrame if a list or array is passed or a grouped Series if only a single column name is passed as a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grouped = df.groupby(['key1', 'key2'])['data2']\n",
    "s_grouped\n",
    "s_grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Dicts and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "                      columns=['a', 'b', 'c', 'd', 'e'],\n",
    "                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose I have a group correspondence for the columns and want to sum together the columns by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\n",
    "           'd': 'blue', 'e': 'red', 'f' : 'orange'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you could construct an array from this dict to pass to groupby, but instead we can just pass the dict (I included the key 'f' to highlight that unused grouping keys are OK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_column = people.groupby(mapping, axis=1)\n",
    "by_column.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Functions\n",
    "\n",
    "Using Python functions is a more generic way of defining a group mapping compared with a dict or Series. Any function passed as a group key will be called once per index value, with the return values being used as the group namesSuppose you wanted to group by the length of the names; while you could compute an array of string lengths, it’s simpler to just pass the len function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = ['one', 'one', 'one', 'two', 'two']\n",
    "people.groupby([len, key_list]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Index Levels\n",
    "\n",
    "A final convenience for hierarchically indexed datasets is the ability to aggregate using one of the levels of an axis index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\n",
    "                                    [1, 3, 5, 1, 3]],\n",
    "                                    names=['cty', 'tenor'])\n",
    "hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)\n",
    "hier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_df.groupby(level='cty', axis=1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation\n",
    "\n",
    "Aggregations refer to any data transformation that produces scalar values from arrays. An aggregated function returns a single aggregated value for each group. Once the **group by**  object is created, several aggregation operations can be performed on the grouped data. \n",
    "\n",
    "The preceding examples have used several of them, including mean, count, min, and sum. You may wonder what is going on when you invoke mean() on a GroupBy object. Many common aggregations, such as those found in Table 10-1, have optimized implementations. However, you are not limited to only this set of methods.\n",
    "\n",
    "\n",
    "\n",
    "| Function name |   Description|\n",
    "|:---|:---|\n",
    "|   count | Number of non-NA values in the group |\n",
    "|   sum|  Sum of non-NA values |\n",
    "|   mean |  Mean of non-NA values |\n",
    "|   median |  Arithmetic median of non-NA values |\n",
    "|   std, var | Unbiased (n – 1 denominator) standard deviation and variance |\n",
    "|   min,max|  Minimum and maximum of non-NA values|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "         'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('Team'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View Groups \n",
    "print(df.groupby('Team').groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "\n",
    "(df.groupby(['Team','Year']).groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('Team')['Points'].agg(np.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Mutliple Aggregation Functions at Once\n",
    "\n",
    "print(df.groupby('Team')['Points'].agg([np.mean, np.sum, np.std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "score = lambda x: (x - x.mean()) / x.std()*10\n",
    "print(df.groupby('Team').transform(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtration: Return the team which have particiapted three or more times\n",
    "print(df.groupby('Team').filter(lambda x:len(x)>=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply: General split-apply-combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.read_csv('/Users/home/Desktop/Univ/UCB/python_for_data_analysis-book-2nd-edition/examples/tips.csv')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tip percentage of total bill\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the tipping dataset from before, suppose you wanted to select the top five tip_pct values by group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(df, n=5, column='tip_pct'):\n",
    "    return df.sort_values(by=column)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top(tips, n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby('smoker').apply(top)  # group sby smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tips.groupby('smoker')['tip_pct'].describe()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack('smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppressing the Group Keys\n",
    "\n",
    "You can disable this by passing group_keys=False to groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby('smoker', group_keys=False).apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering groups\n",
    "\n",
    "\n",
    "The pandas groupby provides a *filter()* method, which can be used to make group level decisions on whether or not the entire group is included in the result after combining. The function passed to *filter()* should return *True* if the group is to be included in teh result and *False* to exclude it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame ({'Label': list(\"AABCCC\"), 'Values': [1,2,3,4,np.nan,5]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are omitting which has minimum number of items. In the example we are omitting the items which have one or less items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x.Values.count() >1\n",
    "df.groupby('Label').filter(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teh following will omit groups that do not have all values supplied\n",
    "f1 = lambda x:x.Values.isnull().sum()  == 0\n",
    "df.groupby('Label').filter(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping a group, the use of the dropna = False parameter allows the return\n",
    "# of the offending groups, but with all their values replaced with NaN. This is useful\n",
    "# if you want to determine which items have been omitted:\n",
    "\n",
    "f = lambda x:x.Values.count() > 1\n",
    "df.groupby('Label').filter(f, dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization and Standardization\n",
    "\n",
    "https://towardsdatascience.com/data-normalization-with-pandas-and-scikit-learn-7c1cc6ed6475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T17:52:21.499148Z",
     "start_time": "2020-10-15T17:52:21.472231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   salary  experience\n",
       "0  100000           5\n",
       "1  125000           7\n",
       "2  175000          10\n",
       "3  250000          20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[100000, 5],[125000, 7],[175000, 10],[250000, 20]], columns = ['salary', 'experience'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T18:15:31.315004Z",
     "start_time": "2020-10-15T18:15:31.010395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     salary  experience\n",
       "0  0.000000    0.000000\n",
       "1  0.166667    0.133333\n",
       "2  0.500000    0.333333\n",
       "3  1.000000    1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "df_normalized = min_max_scaling(df)\n",
    "\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T18:26:19.090442Z",
     "start_time": "2020-10-15T18:26:16.715698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     salary  experience\n",
       "0  0.000000    0.000000\n",
       "1  0.166667    0.133333\n",
       "2  0.500000    0.333333\n",
       "3  1.000000    1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "# fit and transform the data\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T18:28:54.031439Z",
     "start_time": "2020-10-15T18:28:54.014814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5e+05, 2.0e+01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimum values for normalizing the data\n",
    "scaler.data_min_\n",
    "# array([1.2e+05, 1.0e+01])\n",
    "\n",
    "# maximum values for normalizing the data\n",
    "scaler.data_max_\n",
    "# array([4.0e+05, 1.7e+01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T18:54:43.329234Z",
     "start_time": "2020-10-15T18:54:43.293396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     salary  experience\n",
       "0 -0.944911   -0.826033\n",
       "1 -0.566947   -0.525657\n",
       "2  0.188982   -0.075094\n",
       "3  1.322876    1.426785"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
    "def z_score(df):\n",
    "    # copy the dataframe\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "        \n",
    "    return df_std\n",
    "    \n",
    "# call the z_score function\n",
    "df_standardized = z_score(df)\n",
    "\n",
    "df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T18:57:37.172369Z",
     "start_time": "2020-10-15T18:57:37.136863Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/home/anaconda3/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     salary  experience\n",
       "0 -1.091089   -0.953821\n",
       "1 -0.654654   -0.606977\n",
       "2  0.218218   -0.086711\n",
       "3  1.527525    1.647509"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler\n",
    "# fit and transform the data\n",
    "df_std = pd.DataFrame(std_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T19:10:46.378069Z",
     "start_time": "2020-10-15T19:10:46.372008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.72821962e+04 5.76628130e+00]\n",
      "[1.625e+05 1.050e+01]\n"
     ]
    }
   ],
   "source": [
    "#standard deviation for standardizing the data\n",
    "print(std_scaler.scale_)\n",
    "\n",
    "# mean for standardizing the data\n",
    "print(std_scaler.mean_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
